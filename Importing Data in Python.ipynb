{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file with read-only\n",
    "file = open('/Users/serhansari/Desktop/git push.rtf','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing entire text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1504\\\\cocoasubrtf830\\n{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0 Helvetica;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n{\\\\*\\\\expandedcolortbl;;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n\\n\\\\f0\\\\fs24 \\\\cf0 1. Create a repository in Git\\\\\\n2. Make a directory in local\\\\\\n3. Place the .ipynb file at directory location\\\\\\n4. cd to the above directory \\\\\\n5. git init\\\\\\n6. git status\\\\\\n7. git remote add origin https://github.com/ankurv857/Matplotlib.git\\\\\\n8. git status\\\\\\n9. git add Matplotlb.ipynb\\\\\\n10. git commit -m \"initial commit\"\\\\\\n11. git push origin master}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing text files line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1504\\\\cocoasubrtf830\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0 Helvetica;}\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n{\\\\*\\\\expandedcolortbl;;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n\\n\\\\f0\\\\fs24 \\\\cf0 1. Create a repository in Git\\\\\\n2. Make a directory in local\\\\\\n3. Place the .ipynb file at directory location\\\\\\n4. cd to the above directory \\\\\\n5. git init\\\\\\n6. git status\\\\\\n7. git remote add origin https://github.com/ankurv857/Matplotlib.git\\\\\\n8. git status\\\\\\n9. git add Matplotlb.ipynb\\\\\\n10. git commit -m \"initial commit\"\\\\\\n11. git push origin master}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.read() # Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file.close() # Close File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\\rtf1\\ansi\\ansicpg1252\\cocoartf1504\\cocoasubrtf830\n",
      "{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n",
      "{\\colortbl;\\red255\\green255\\blue255;}\n",
      "{\\*\\expandedcolortbl;;}\n",
      "\\margl1440\\margr1440\\vieww10800\\viewh8400\\viewkind0\n",
      "\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n",
      "\n",
      "\\f0\\fs24 \\cf0 1. Create a repository in Git\\\n",
      "2. Make a directory in local\\\n",
      "3. Place the .ipynb file at directory location\\\n",
      "4. cd to the above directory \\\n",
      "5. git init\\\n",
      "6. git status\\\n",
      "7. git remote add origin https://github.com/ankurv857/Matplotlib.git\\\n",
      "8. git status\\\n",
      "9. git add Matplotlb.ipynb\\\n",
      "10. git commit -m \"initial commit\"\\\n",
      "11. git push origin master}\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/serhansari/Desktop/git push.rtf') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NumPy to import flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', 'Chocolate Sandwich Cookies'],\n",
       "       ['2', 'All-Seasons Salt'],\n",
       "       ['3', 'Robust Golden Unsweetened Oolong Tea'],\n",
       "       ..., \n",
       "       ['49686', 'Artisan Baguette'],\n",
       "       ['49687', 'Smartblend Healthy Metabolism Dry Cat Food'],\n",
       "       ['49688', 'Fresh Foaming Cleanser']], \n",
       "      dtype='|S159')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.loadtxt()\n",
    "\n",
    "file = '/Users/serhansari/Desktop/instacart_2017_05_01/products.csv'\n",
    "np.loadtxt(file, delimiter=',', dtype=str , skiprows=1, usecols=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with mixed datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('1,Chocolate Sandwich Cookies,61,19',),\n",
       "       ('2,All-Seasons Salt,104,13',),\n",
       "       ('3,Robust Golden Unsweetened Oolong Tea,94,7',), ...,\n",
       "       ('49686,Artisan Baguette,112,3',),\n",
       "       ('49687,Smartblend Healthy Metabolism Dry Cat Food,41,8',),\n",
       "       ('49688,Fresh Foaming Cleanser,73,11',)], \n",
       "      dtype=[('product_idproduct_nameaisle_iddepartment_id', 'S171')])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.genfromtxt()\n",
    "\n",
    "np.genfromtxt(file, delimiter='\\t', names=True, dtype=None)\n",
    "\n",
    "# Here, the first argument is the filename, the second specifies the delimiter , and the third argument names tells us there is a header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49688,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([('1,Chocolate Sandwich Cookies,61,19',), ('2,All-Seasons Salt,104,13',),\n",
       " ('3,Robust Golden Unsweetened Oolong Tea,94,7',), ...,\n",
       " ('49686,Artisan Baguette,112,3',),\n",
       " ('49687,Smartblend Healthy Metabolism Dry Cat Food,41,8',),\n",
       " ('49688,Fresh Foaming Cleanser,73,11',)], \n",
       "          dtype=[('product_idproduct_nameaisle_iddepartment_id', 'S171')])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.recfromcsv() -> default dtype=None\n",
    "\n",
    "np.recfromcsv(file,delimiter='\\t',names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing flat files using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product_id</td>\n",
       "      <td>product_name</td>\n",
       "      <td>aisle_id</td>\n",
       "      <td>department_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chocolate Sandwich Cookies</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>All-Seasons Salt</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Robust Golden Unsweetened Oolong Tea</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Smart Ones Classic Favorites Mini Rigatoni Wit...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                                  1         2  \\\n",
       "0  product_id                                       product_name  aisle_id   \n",
       "1           1                         Chocolate Sandwich Cookies        61   \n",
       "2           2                                   All-Seasons Salt       104   \n",
       "3           3               Robust Golden Unsweetened Oolong Tea        94   \n",
       "4           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
       "\n",
       "               3  \n",
       "0  department_id  \n",
       "1             19  \n",
       "2             13  \n",
       "3              7  \n",
       "4              1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the first 5 rows of the file into a DataFrame\n",
    "df = pd.read_csv(file, nrows=5, header=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['product_id', 'product_name', 'aisle_id', 'department_id'],\n",
       "       ['1', 'Chocolate Sandwich Cookies', '61', '19'],\n",
       "       ['2', 'All-Seasons Salt', '104', '13'],\n",
       "       ['3', 'Robust Golden Unsweetened Oolong Tea', '94', '7'],\n",
       "       ['4',\n",
       "        'Smart Ones Classic Favorites Mini Rigatoni With Vodka Cream Sauce',\n",
       "        '38', '1']], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a numpy array from the DataFrame\n",
    "data_array = df.values\n",
    "\n",
    "data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the sep (the pandas version of delim), comment and na_values arguments of pd.read_csv(). comment takes characters that comments occur after in the file, which in this case is '#'. na_values takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the sep (the pandas version of delim), comment and na_values arguments of pd.read_csv(). comment takes characters that comments occur after in the file, which in this case is '#'. na_values takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.\n",
    "data = pd.read_csv(file, sep=',', comment='#', na_values='Nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chocolate Sandwich Cookies</td>\n",
       "      <td>61.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All-Seasons Salt</td>\n",
       "      <td>104.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Robust Golden Unsweetened Oolong Tea</td>\n",
       "      <td>94.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Smart Ones Classic Favorites Mini Rigatoni Wit...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Green Chile Anytime Sauce</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  aisle_id  \\\n",
       "0           1                         Chocolate Sandwich Cookies      61.0   \n",
       "1           2                                   All-Seasons Salt     104.0   \n",
       "2           3               Robust Golden Unsweetened Oolong Tea      94.0   \n",
       "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...      38.0   \n",
       "4           5                          Green Chile Anytime Sauce       5.0   \n",
       "\n",
       "   department_id  \n",
       "0           19.0  \n",
       "1           13.0  \n",
       "2            7.0  \n",
       "3            1.0  \n",
       "4           13.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library os"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first line of the following code imports the library os, the second line stores the name of the current directory in a string called wd and the third outputs the contents of the directory in a list to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'Importing Data in Python.ipynb',\n",
       " 'numpy.ipynb',\n",
       " 'python.ipynb']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pickled file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing sheets in Excel files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "given an Excel file imported into a variable spreadsheet, you can retrieve a list of the sheet names using the attribute spreadsheet.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bdonly', u'Serhan', u'Sari']\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = '/Users/serhansari/Desktop/prio.xls'\n",
    "\n",
    "# Load spreadsheet: xl\n",
    "xl = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing sheets from Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "   id  year  bdeadlow  bdeadhig  bdeadbes  annualdata  source  bdversion  \\\n",
      "0   1  1946      1000      9999      1000           2       1        3.1   \n",
      "1   1  1952       450      3000      -999           2       1        3.1   \n",
      "2   1  1967        25       999        82           2       1        3.1   \n",
      "3   2  1946        25       999      -999           0       0        3.1   \n",
      "4   2  1947        25       999      -999           0       0        3.1   \n",
      "\n",
      "   location    sidea   ...    epend  ependdate ependprec  gwnoa gwnoa2nd  \\\n",
      "0   Bolivia  Bolivia   ...        1 1946-07-21     -99.0    145      NaN   \n",
      "1   Bolivia  Bolivia   ...        1 1952-04-12     -99.0    145      NaN   \n",
      "2   Bolivia  Bolivia   ...        1 1967-10-16     -99.0    145      NaN   \n",
      "3  Cambodia   France   ...        0        NaT       NaN    220      NaN   \n",
      "4  Cambodia   France   ...        0        NaT       NaN    220      NaN   \n",
      "\n",
      "   gwnob  gwnob2nd  gwnoloc region  version  \n",
      "0    NaN       NaN      145      5   2009-4  \n",
      "1    NaN       NaN      145      5   2009-4  \n",
      "2    NaN       NaN      145      5   2009-4  \n",
      "3    NaN       NaN      811      3   2009-4  \n",
      "4    NaN       NaN      811      3   2009-4  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xl.parse('Sari')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xl.parse('bdonly')\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing your spreadsheet import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "additional arguments skiprows, names and parse_cols. These skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists containing the specific row numbers, strings and column numbers, as appropriate."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df3 = xl.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df3.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df4 = xl.parse(1, parse_cols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.excel.ExcelFile at 0x11af97fd0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing SAS files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- SAS: Statistical Analysis System \n",
    "- SAS: business analytics and biostatistics \n",
    "\n",
    "Used for:\n",
    "- Advanced analytics\n",
    "- Multivariate analysis \n",
    "- Business intelligence \n",
    "- Data management \n",
    "- Predictive analytics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import a SAS file as a DataFrame using SAS7BDAT and pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using read_stata to import Stata files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Stata: “Statistics” + “data”\n",
    "- Stata: academic social sciences research"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_stata('disarea.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Hierarchical Data Format version 5\n",
    "- Standard for storing large quantities of numerical data \n",
    "- Datasets can be hundreds of gigabytes or terabytes \n",
    "- HDF5 can scale to exabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py to import HDF5 files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    <class 'h5py._hl.files.File'>\n",
    "    meta\n",
    "    quality\n",
    "    strain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatLab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- “Matrix Laboratory”\n",
    "- Industry standard in engineering and science \n",
    "- Data saved as .mat files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.io.loadmat() - read .mat files \n",
    "scipy.io.savemat() - write .mat files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading .mat files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    <class 'dict'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to relational database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a database engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Engine(sqlite:///Chinook.sqlite)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print table_names\n",
    "\n",
    "print engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying relational databases in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow of SQL querying\n",
    "1. Import packages and functions \n",
    "2. Create the database engine \n",
    "3. Connect to the engine\n",
    "4. Query the database\n",
    "5. Save query results to a DataFrame \n",
    "6. Close the connection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "       0                                      1  2\n",
    "    0  1  For Those About To Rock We Salute You  1\n",
    "    1  2                      Balls to the Wall  2\n",
    "    2  3                      Restless and Wild  2\n",
    "    3  4                      Let There Be Rock  1\n",
    "    4  5                               Big Ones  3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the SQL Queries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Customize Query in order to:\n",
    "\n",
    "- Select specified columns from a table;\n",
    "- Select a specified number of rows;\n",
    "- Import column names from the database table."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT LastName, Title FROM Employee')\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    3\n",
    "      LastName                Title\n",
    "    0    Adams      General Manager\n",
    "    1  Edwards        Sales Manager\n",
    "    2  Peacock  Sales Support Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering your database records using SQL's WHERE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT * FROM Customer WHERE Country = 'Canada'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT * FROM Employee WHERE EmployeeId >= 6')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "       EmployeeId  LastName FirstName       Title  ReportsTo            BirthDate  \\\n",
    "    0           6  Mitchell   Michael  IT Manager          1  1973-07-01 00:00:00   \n",
    "    1           7      King    Robert    IT Staff          6  1970-05-29 00:00:00   \n",
    "    2           8  Callahan     Laura    IT Staff          6  1968-01-09 00:00:00   \n",
    "    \n",
    "                  HireDate                      Address        City State Country  \\\n",
    "    0  2003-10-17 00:00:00         5827 Bowness Road NW     Calgary    AB  Canada   \n",
    "    1  2004-01-02 00:00:00  590 Columbia Boulevard West  Lethbridge    AB  Canada   \n",
    "    2  2004-03-04 00:00:00                  923 7 ST NW  Lethbridge    AB  Canada   \n",
    "    \n",
    "      PostalCode              Phone                Fax                    Email  \n",
    "    0    T3B 0C5  +1 (403) 246-9887  +1 (403) 246-9899  michael@chinookcorp.com  \n",
    "    1    T1K 5N8  +1 (403) 456-9986  +1 (403) 456-8485   robert@chinookcorp.com  \n",
    "    2    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772    laura@chinookcorp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering your SQL records with ORDER BY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT * FROM Customer ORDER BY SupportRepId"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT * FROM Employee ORDER BY BirthDate')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "<script.py> output:\n",
    "       EmployeeId  LastName FirstName                Title  ReportsTo  \\\n",
    "    0           4      Park  Margaret  Sales Support Agent        2.0   \n",
    "    1           2   Edwards     Nancy        Sales Manager        1.0   \n",
    "    2           1     Adams    Andrew      General Manager        NaN   \n",
    "    3           5   Johnson     Steve  Sales Support Agent        2.0   \n",
    "    4           8  Callahan     Laura             IT Staff        6.0   \n",
    "    \n",
    "                 BirthDate             HireDate              Address        City  \\\n",
    "    0  1947-09-19 00:00:00  2003-05-03 00:00:00     683 10 Street SW     Calgary   \n",
    "    1  1958-12-08 00:00:00  2002-05-01 00:00:00         825 8 Ave SW     Calgary   \n",
    "    2  1962-02-18 00:00:00  2002-08-14 00:00:00  11120 Jasper Ave NW    Edmonton   \n",
    "    3  1965-03-03 00:00:00  2003-10-17 00:00:00         7727B 41 Ave     Calgary   \n",
    "    4  1968-01-09 00:00:00  2004-03-04 00:00:00          923 7 ST NW  Lethbridge   \n",
    "    \n",
    "      State Country PostalCode              Phone                Fax  \\\n",
    "    0    AB  Canada    T2P 5G3  +1 (403) 263-4423  +1 (403) 263-4289   \n",
    "    1    AB  Canada    T2P 2T3  +1 (403) 262-3443  +1 (403) 262-3322   \n",
    "    2    AB  Canada    T5K 2N1  +1 (780) 428-9482  +1 (780) 428-3457   \n",
    "    3    AB  Canada    T3B 1Y7   1 (780) 836-9987   1 (780) 836-9543   \n",
    "    4    AB  Canada    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772   \n",
    "    \n",
    "                          Email  \n",
    "    0  margaret@chinookcorp.com  \n",
    "    1     nancy@chinookcorp.com  \n",
    "    2    andrew@chinookcorp.com  \n",
    "    3     steve@chinookcorp.com  \n",
    "    4     laura@chinookcorp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas and the SQL Queries!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query('SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate',engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result: does df = df1 ?\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "       AlbumId                                  Title  ArtistId\n",
    "    0        1  For Those About To Rock We Salute You         1\n",
    "    1        2                      Balls to the Wall         2\n",
    "    2        3                      Restless and Wild         2\n",
    "    3        4                      Let There Be Rock         1\n",
    "    4        5                               Big Ones         3\n",
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships between tables: INNER JOIN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "                                       Title       Name\n",
    "    0  For Those About To Rock We Salute You      AC/DC\n",
    "    1                      Balls to the Wall     Accept\n",
    "    2                      Restless and Wild     Accept\n",
    "    3                      Let There Be Rock      AC/DC\n",
    "    4                               Big Ones  Aerosmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering to INNER JOIN with pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query('SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000',engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "       PlaylistId  TrackId  TrackId              Name  AlbumId  MediaTypeId  \\\n",
    "    0           1     3390     3390  One and the Same      271            2   \n",
    "    1           1     3392     3392     Until We Fall      271            2   \n",
    "    2           1     3393     3393     Original Fire      271            2   \n",
    "    3           1     3394     3394       Broken City      271            2   \n",
    "    4           1     3395     3395          Somedays      271            2   \n",
    "    \n",
    "       GenreId Composer  Milliseconds    Bytes  UnitPrice  \n",
    "    0       23     None        217732  3559040       0.99  \n",
    "    1       23     None        230758  3766605       0.99  \n",
    "    2       23     None        218916  3577821       0.99  \n",
    "    3       23     None        228366  3728955       0.99  \n",
    "    4       23     None        213831  3497176       0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing flat files from the web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The urllib package\n",
    "- Provides interface for fetching data across the web urlopen() \n",
    "- accepts URLs instead of file names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Require Python 3\n",
    "\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url,'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and reading flat files from the web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the function pd.read_csv() with the URL as the first argument and the separator sep as the second argument."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Require Python 3\n",
    "\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing non-flat files from the web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xl\n",
    "xl = pd.read_excel(url, sheetname=None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xl['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP requests to import files from the web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTTP: HyperText Transfer Protocol\n",
    "Foundation of data communication for the web\n",
    "HTTPS - more secure form of HTTP\n",
    "Going to a website = sending HTTP request\n",
    "GET request\n",
    "urlretrieve() performs a GET request HTML - HyperText Markup Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing HTTP requests in Python using urllib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    <class 'http.client.HTTPResponse'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing HTTP request results in Python using urllib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Packaged and sent a GET request to \"http://www.datacamp.com/teach/documentation\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated read() method. You'll build on your previous great work to extract the response and print the HTML."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    b'<!DOCTYPE html>\\n<html>\\n\\t<head>\\n\\n\\t\\t<link rel=\"stylesheet\" media=\"all\" href=\"/teach/assets/application-9f12ba0d57351944af51615a03f2adabd8bba1085fe02997bb742232f9592cc3.css\" />\\n\\t\\t<link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/teach/assets/favicon-335cd0394b32102a39221d79e5fd7e51078e6d32a0c8aea59676a6869f84e9d8.ico\" />\\n\\t\\t<meta name=\"csrf-param\" content=\"authenticity_token\" />\\n<meta name=\"csrf-token\" content=\"fm0KKNptQOgiQ/rNytApgqkoySurwDMjai99+dwBhWX9vkro7fn4O2Vh+E3dcVu6znbvXIJeHwk/oqKJ8ktGVA==\" />\\n\\n\\t\\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"4795905ee2\",\"applicationID\":\"13547831\",\"transactionName\":\"JlkNEEQLVA0DE0wVFwRCCgdpFFkGAxJMAgwGQw4BWBBZFQ8ODQ==\",\"queueTime\":0,\"applicationTime\":42,\"agent\":\"\"}</script>\\n<script type=\"text/javascript\">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing HTTP requests in Python using requests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How to do the same using the higher-level requests library"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    \t<head>\n",
    "    \n",
    "    \t\t<link rel=\"stylesheet\" media=\"all\" href=\"/teach/assets/application-9f12ba0d57351944af51615a03f2adabd8bba1085fe02997bb742232f9592cc3.css\" />\n",
    "    \t\t<link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/teach/assets/favicon-335cd0394b32102a39221d79e5fd7e51078e6d32a0c8aea59676a6869f84e9d8.ico\" />\n",
    "    \t\t<meta name=\"csrf-param\" content=\"authenticity_token\" />\n",
    "    <meta name=\"csrf-token\" content=\"LiH9YWWUddas2VF5u00jcqO8uEYIYtbhlolpHdq58iJycopa/1b5NcWNvPxEkf0vo3OF5nypZmcAahAIjvKOfw==\" />\n",
    "    \n",
    "    \t\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the web in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "Parse and extract structured data from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   <a href=\"http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\">\n",
      "    Who\n",
      "I Am\n",
      "   </a>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Read\n",
      "my\n",
      "   <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "    \"King's\n",
      "Day Speech\"\n",
      "   </a>\n",
      "   for some inspiration.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am the author of the\n",
      "   <a href=\"http://www.python.org\">\n",
      "    Python\n",
      "   </a>\n",
      "   programming language.  See also my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   and my\n",
      "   <a href=\"Publications.html\">\n",
      "    publications list\n",
      "   </a>\n",
      "   , a\n",
      "   <a href=\"bio.html\">\n",
      "    brief bio\n",
      "   </a>\n",
      "   , assorted\n",
      "   <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "    writings\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "    presentations\n",
      "   </a>\n",
      "   and\n",
      "   <a href=\"interviews.html\">\n",
      "    interviews\n",
      "   </a>\n",
      "   (all about Python), some\n",
      "   <a href=\"pics.html\">\n",
      "    pictures of me\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://neopythonic.blogspot.com\">\n",
      "    my new blog\n",
      "   </a>\n",
      "   , and\n",
      "my\n",
      "   <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "    old\n",
      "blog\n",
      "   </a>\n",
      "   on Artima.com.  I am\n",
      "   <a href=\"https://twitter.com/gvanrossum\">\n",
      "    @gvanrossum\n",
      "   </a>\n",
      "   on Twitter.  I\n",
      "also have\n",
      "a\n",
      "   <a href=\"https://plus.google.com/u/0/115212051037621986145/posts\">\n",
      "    G+\n",
      "profile\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <p>\n",
      "   In January 2013 I joined\n",
      "   <a href=\"http://www.dropbox.com\">\n",
      "    Dropbox\n",
      "   </a>\n",
      "   .  I work on various Dropbox\n",
      "products and have 50% for my Python work, no strings attached.\n",
      "Previously, I have worked for Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   .)  I created Python while at CWI.\n",
      "  </p>\n",
      "  <h3>\n",
      "   How to Reach Me\n",
      "  </h3>\n",
      "  <p>\n",
      "   You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but if you ask\n",
      "me a question about using Python, it's likely that I won't have time\n",
      "to answer it, and will instead refer you to\n",
      "help (at) python.org,\n",
      "   <a href=\"http://groups.google.com/groups?q=comp.lang.python\">\n",
      "    comp.lang.python\n",
      "   </a>\n",
      "   or\n",
      "   <a href=\"http://stackoverflow.com\">\n",
      "    StackOverflow\n",
      "   </a>\n",
      "   .  If you need to\n",
      "talk to me on the phone or send me something by snail mail, send me an\n",
      "email and I'll gladly email you instructions on how to reach me.\n",
      "  </p>\n",
      "  <h3>\n",
      "   My Name\n",
      "  </h3>\n",
      "  <p>\n",
      "   My name often poses difficulties for Americans.\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Pronunciation:\n",
      "   </b>\n",
      "   in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "   <a href=\"guido.au\">\n",
      "    sound clip\n",
      "   </a>\n",
      "   .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Spelling:\n",
      "   </b>\n",
      "   my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Alphabetization:\n",
      "   </b>\n",
      "   in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "  </p>\n",
      "  <h3>\n",
      "   More Hyperlinks\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    Here's a collection of\n",
      "    <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "     essays\n",
      "    </a>\n",
      "    relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "   <li>\n",
      "    I own the official\n",
      "    <a href=\"images/license.jpg\">\n",
      "     <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "     Python license.\n",
      "    </a>\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <h3>\n",
      "   The Audio File Formats FAQ\n",
      "  </h3>\n",
      "  <p>\n",
      "   I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "   <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "    http://www.cnpbagwell.com/audio-faq\n",
      "   </a>\n",
      "   .  And here is a link to\n",
      "   <a href=\"http://sox.sourceforge.net/\">\n",
      "    SOX\n",
      "   </a>\n",
      "   , to which I contributed\n",
      "some early code.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <a href=\"images/internetdog.gif\">\n",
      "   \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "  </a>\n",
      "  <hr/>\n",
      " </body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/serhansari/anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/serhansari/anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning a webpage into data using BeautifulSoup: getting the text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "how to extract the text from the BDFL's webpage, along with printing the webpage's title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "\n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\"Gawky and proud of it.\"\n",
      "Who\n",
      "I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.  I\n",
      "also have\n",
      "a G+\n",
      "profile.\n",
      "\n",
      "In January 2013 I joined\n",
      "Dropbox.  I work on various Dropbox\n",
      "products and have 50% for my Python work, no strings attached.\n",
      "Previously, I have worked for Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but if you ask\n",
      "me a question about using Python, it's likely that I won't have time\n",
      "to answer it, and will instead refer you to\n",
      "help (at) python.org,\n",
      "comp.lang.python or\n",
      "StackOverflow.  If you need to\n",
      "talk to me on the phone or send me something by snail mail, send me an\n",
      "email and I'll gladly email you instructions on how to reach me.\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning a webpage into data using BeautifulSoup: getting the hyperlinks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "how to extract the URLs of the hyperlinks from the BDFL's webpage with the soup method find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "https://plus.google.com/u/0/115212051037621986145/posts\n",
      "http://www.dropbox.com\n",
      "Resume.html\n",
      "http://groups.google.com/groups?q=comp.lang.python\n",
      "http://stackoverflow.com\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to APIs and JSONs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "APIs:\n",
    "- Application Programming Interface \n",
    "- Protocols and routines\n",
    "- Building and interacting with software applications"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "JSONs:\n",
    "- JavaScript Object Notation\n",
    "- Real-time server-to-browser communication \n",
    "- Douglas Crockford\n",
    "- Human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and exploring a JSON"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "with open('snakes.json', 'r') as json_file:\n",
    "     json_data = json.load(json_file)\n",
    "\n",
    "type(json_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "(u'info: ', {u'termsOfService': u'http://swagger.io/terms/', u'description': u'Information regarding AU Hackathon-APIs', u'license': {u'url': u'http://www.apache.org/licenses/LICENSE-2.0.html', u'name': u'Capital One'}, u'title': u'AU Hackathon APIs', u'contact': {u'email': u'mohit.gadkari@capitalone.com'}, u'version': u'1.0.0'})\n",
      "(u'paths: ', {u'/au-hackathon/payments/': {u'post': {u'responses': {u'200': {u'description': u'successful operation', u'schema': {u'items': {u'$ref': u'#/definitions/response'}, u'type': u'array'}}, u'400': {u'description': u'Invalid status value'}}, u'produces': [u'application/json'], u'description': u'return current payments', u'parameters': [{u'schema': {u'$ref': u'#/definitions/au_payments'}, u'description': u'Payments post body', u'required': False, u'name': u'body', u'in': u'body'}], u'summary': u'Get Payments'}}})\n",
      "(u'schemes: ', [u'https'])\n",
      "(u'tags: ', [{u'name': u'Payments API', u'description': u'For Payments'}])\n",
      "(u'basePath: ', u'/prod')\n",
      "(u'host: ', u'3hkaob4gkc.execute-api.us-east-1.amazonaws.com')\n",
      "(u'definitions: ', {u'response_payment': {u'properties': {u'total_balance': {u'type': u'integer', u'description': u'total balance'}, u'year': {u'type': u'integer', u'description': u'current year'}, u'total_balance_remaining': {u'type': u'integer', u'description': u'balance remaining'}, u'total_balance_paid': {u'type': u'integer', u'description': u'balance paid'}, u'month': {u'type': u'string', u'description': u'current month'}}}, u'au_payments': {u'properties': {u'month_year_from': {u'type': u'integer', u'description': u'Start date for rewards', u'format': u'date'}, u'account_id': {u'type': u'integer', u'description': u'Account id'}, u'month_year_to': {u'type': u'integer', u'description': u'End date for rewards', u'format': u'date'}}}, u'response': {u'properties': {u'account_id': {u'type': u'integer', u'description': u'customer account Id'}, u'payment': {u'$ref': u'#/definitions/response_payment'}}}})\n",
      "(u'swagger: ', u'2.0')\n",
      "(u'externalDocs: ', {u'url': u'http://swagger.io', u'description': u'Find out more about Swagger'})\n"
     ]
    }
   ],
   "source": [
    "# Load JSON: json_data\n",
    "import json\n",
    "with open(\"/Users/serhansari/Desktop/swagger.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    \n",
    "print(type(json_data))\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs and interacting with the world wide web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- An API is a set of protocols and routines for building and interacting with software applications.\n",
    "- API is an acronym and is short for Application Program interface.\n",
    "- An API is a bunch of code that allows two software programs to communicate with each other.\n",
    "- It is common to pull data from APIs in the JSON file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin (screenplay), Ben Mezrich (book)\",\"Actors\":\"Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons\",\"Plot\":\"Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"USA\",\"Awards\":\"Won 3 Oscars. Another 165 wins & 168 nominations.\",\"Poster\":\"https://images-na.ssl-images-amazon.com/images/M/MV5BMTM2ODk0NDAwMF5BMl5BanBnXkFtZTcwNTM1MDc2Mw@@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.7/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.7\",\"imdbVotes\":\"522,984\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"11 Jan 2011\",\"BoxOffice\":\"$96,400,000\",\"Production\":\"Columbia Pictures\",\"Website\":\"http://www.thesocialnetwork-movie.com/\",\"Response\":\"True\"}\n"
     ]
    }
   ],
   "source": [
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable and add the apikey with /?\n",
    "url = ('http://www.omdbapi.com/?apikey=ff21610b&t=social+network')\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON–from the web to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Plot: ', u'Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.')\n",
      "(u'Rated: ', u'PG-13')\n",
      "(u'Title: ', u'The Social Network')\n",
      "(u'Ratings: ', [{u'Source': u'Internet Movie Database', u'Value': u'7.7/10'}, {u'Source': u'Rotten Tomatoes', u'Value': u'96%'}, {u'Source': u'Metacritic', u'Value': u'95/100'}])\n",
      "(u'DVD: ', u'11 Jan 2011')\n",
      "(u'Writer: ', u'Aaron Sorkin (screenplay), Ben Mezrich (book)')\n",
      "(u'Production: ', u'Columbia Pictures')\n",
      "(u'Actors: ', u'Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons')\n",
      "(u'Type: ', u'movie')\n",
      "(u'imdbVotes: ', u'522,984')\n",
      "(u'Website: ', u'http://www.thesocialnetwork-movie.com/')\n",
      "(u'Poster: ', u'https://images-na.ssl-images-amazon.com/images/M/MV5BMTM2ODk0NDAwMF5BMl5BanBnXkFtZTcwNTM1MDc2Mw@@._V1_SX300.jpg')\n",
      "(u'Director: ', u'David Fincher')\n",
      "(u'Released: ', u'01 Oct 2010')\n",
      "(u'Awards: ', u'Won 3 Oscars. Another 165 wins & 168 nominations.')\n",
      "(u'Genre: ', u'Biography, Drama')\n",
      "(u'imdbRating: ', u'7.7')\n",
      "(u'Language: ', u'English, French')\n",
      "(u'Country: ', u'USA')\n",
      "(u'BoxOffice: ', u'$96,400,000')\n",
      "(u'Runtime: ', u'120 min')\n",
      "(u'imdbID: ', u'tt1285016')\n",
      "(u'Metascore: ', u'95')\n",
      "(u'Response: ', u'True')\n",
      "(u'Year: ', u'2010')\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the Wikipedia API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n",
    "\n",
    "The URL that requests the relevant query from the Wikipedia API is\n",
    "\n",
    "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Pizza</b> is a yeasted flatbread typically topped with tomato sauce and cheese and baked in an oven. It is commonly topped with a selection of meats, vegetables and condiments.</p>\n",
      "<p>The term <i>pizza</i> was first recorded in the 10th century, in a Latin manuscript from Gaeta in Central Italy. Modern pizza was invented in Naples, Italy, and the dish and its variants have since become popular and common in many areas of the world. In 2009, upon Italy's request, Neapolitan pizza was safeguarded in the European Union as a Traditional Speciality Guaranteed dish. <i>Associazione Verace Pizza Napoletana</i> (True Neapolitan Pizza Association), a non-profit organization founded in 1984 with headquarters in Naples, aims to \"promote and protect... the true Neapolitan pizza\".</p>\n",
      "<p>Pizza is sold fresh or frozen, either whole or in portions, and is a common fast food item in Europe and North America. Various types of ovens are used to cook them and many varieties exist. Several similar dishes are prepared from ingredients commonly used in pizza preparation, such as calzone and stromboli.</p>\n",
      "<p></p>\n",
      "\n",
      "<p></p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Twitter API and Authentication"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# How to stream data from the Twitter API \n",
    "# How to filter incoming tweets for keywords \n",
    "# About API Authentication and OAuth\n",
    "# How to use the Tweepy Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Authentication"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The package tweepy is great at handling all the Twitter API OAuth Authentication details for you. All you need to do is pass it your authentication credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import tweepy\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that you have set up your authentication credentials, it is time to stream some tweets!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.file = open(\"tweets.txt\", \"w\")\n",
    "\n",
    "    def on_status(self, status):\n",
    "        tweet = status._json\n",
    "        self.file.write( json.dumps(tweet) + '\\n' )\n",
    "        self.num_tweets += 1\n",
    "        if self.num_tweets < 100:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        self.file.close()\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create you Stream object with authentication\n",
    "stream = tweepy.Stream(auth, 1)\n",
    "\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track=['clinton','trump','sanders','cruz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and explore your Twitter data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    dict_keys(['id_str', 'coordinates', 'in_reply_to_user_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'filter_level', 'favorited', 'in_reply_to_screen_name', 'user', 'retweet_count', 'retweeted_status', 'source', 'contributors', 'geo', 'lang', 'favorite_count', 'text', 'extended_entities', 'timestamp_ms', 'entities', 'possibly_sensitive', 'created_at', 'in_reply_to_status_id', 'place', 'is_quote_status', 'id', 'retweeted', 'truncated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter data to DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "                                                    text lang\n",
    "    0  b\"RT @bpolitics: .@krollbondrating's Christoph...   en\n",
    "    1  b'RT @HeidiAlpine: @dmartosko Cruz video found...   en\n",
    "    2  b'Njihuni me Zonj\\\\xebn Trump !!! | Ekskluzive...   et\n",
    "    3  b\"Your an idiot she shouldn't have tried to gr...   en\n",
    "    4  b'RT @AlanLohner: The anti-American D.C. elite...   en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit of Twitter text analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def word_in_text(word, tweet):\n",
    "    word = word.lower()\n",
    "    text = tweet.lower()\n",
    "    match = re.search(word, tweet)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
